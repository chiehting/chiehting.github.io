---
date: 2023-07-12T16:14:50+08:00
updated: 2023-07-25T16:49:35+08:00
title: Service Level Ojbective
category: sre
tags: [slo]
type: note
author: Atlassian
status: ğŸŒ±
sourceType: ğŸ“°ï¸
sourceURL: https://www.atlassian.com/blog/opsgenie/measuring-and-evaluating-service-level-objectives
---

### Evergreen Note

Question :: ç‚ºä»€éº¼ SLO é€™éº¼é‡è¦?

Answer :: SLO æ˜¯ç‚ºæ»¿è¶³ SLA çš„ç›®æ¨™è¨­ç«‹, å°‡å®¢æˆ¶å°ç³»çµ±ç©©å®šåº¦çš„æœŸå¾…è½‰æ›æˆç›®æ¨™. ä¹‹æ‰€ä»¥é‡è¦æ˜¯å› ç‚º IT åœ˜éšŠç›´æ¥é—œæ³¨å®¢æˆ¶æ‰€åœ¨ä¹çš„é‡é», è®“ç³»çµ±çš„ç©©å®šåº¦ä¿æŒåœ¨å¯æ¥å—çš„ç¯„åœå…§. ç•¶ç„¶æœå‹™è¶Šå¯é ; æˆæœ¬å°±è¶Šé«˜.

<!--more-->

### Summary

Service level objective (æœå‹™æ°´æº–ç›®æ¨™) æ˜¯ç³»çµ±ç›£æ§çš„ç›®æ¨™, ç‚ºäº†æ»¿è¶³ SLA æ‰€æ‰¿è«¾ä¹‹å”è­°. SLO çš„çµ„æˆæ˜¯ç”± **SLIã€ä¸€æ®µæ™‚é–“å€é–“ã€ç›®æ¨™(é€šå¸¸ä»¥ç™¾åˆ†æ¯”å‘ˆç¾)**, å…¬å¼å¦‚ä¸‹:

$$ SLO = SLI + period\ of\ time + valid\ conditions $$

åœ¨å»ºç«‹ SLOs çš„æ™‚å€™ç›®æ¨™å¿…é ˆæ˜¯æ˜ç¢ºä¸”æ¸…æ™°çš„. åœ¨æ–‡ä¸­å‡ºæŒ‡å‡ºäº”é»ä¾†æ¸¬é‡èˆ‡è©•ä¼° SLOs, æˆ‘å€‹äººçš„ç¶“é©—è£¡, åœ¨åŸ·è¡Œä¸Šæœƒæœ‰äº›å›°é›£é», åœ¨ä¸‹é¢ä¸€ä½µåˆ—å‡º.

- **è¨­å®šæº–ç¢ºçš„ç›®æ¨™**: é ˆå°‡ç›®æ¨™æ”¶æ–‚. è¨­å®šäººéœ€æ˜ç¢ºç›®æ¨™, ä¸è¦éæ–¼ç™¼æ•£, è®“æŒ‡æ¨™ç²¾ç¢ºä»¥åˆ©æ–¼å¾Œè¨±çš„è¡¡é‡.
- **æœé›†ç›£æ§æ•¸æ“š**: æ•¸æ“šéœ€è¦å®šç¾©æ¸…æ¥šä¸”å®Œå–„, å¯è¦–è¦ºåŒ–è¼”åŠ©. å·¥å…·ä¾‹å¦‚: DataDog, Grafana. è¨­ç«‹äººéœ€å°ç›®æ¨™æ¸…æ¥šèˆ‡ç³»çµ±äº†è§£, ä½¿å¾—è³‡è¨Šæœé›†å®Œå…¨ä¸”æ­£ç¢º.
- **å°æ”¶é›†çš„æŒ‡æ¨™ç™¼å‡ºè­¦å ±**: å°æ–¼å‘Šè­¦è¦é€æ˜ä¸”å…¬é–‹å¾ˆèªåŒ, ä½†å¤§é‡çš„è­¦å ±æœƒè®“ IT åœ˜éšŠæ„Ÿå—åˆ°è®Šå¼±. é€™é‚Šå¯ä»¥åšè­¦å ±æ¬Šé‡çš„é…ç½®, è®“ IT åœ˜éšŠå°æ–¼è­¦å ±äº‹ä»¶æ˜¯è¦ç¹ƒç·Šç¥ç¶“çš„. å·¥å…·ä¾‹å¦‚: Opsgenie
- **å»ºç«‹è­¦å ±å ±å‘Š**: å–å¾—äº‹ä»¶çš„æ•¸æ“šå ±å‘Š, åŒ…æ‹¬æ¯é …æœå‹™è§£æ±ºå’Œé—œé–‰äº‹ä»¶çš„å¹³å‡æ™‚é–“ã€æœå‹™å¥åº·ç™¾åˆ†æ¯”ã€äº‹ä»¶çš„åš´é‡æ€§ã€äº‹ä»¶çš„é—œè¯æ€§ç­‰ç­‰. é—œéµçš„æ•¸æ“šå ±å‘Šå¯ä»¥è®“è©•ä¼°æº–ç¢º.
- **å ±å‘Šçš„è©•ä¼°èˆ‡åˆ†äº«**: è­¦å ±äº‹ä»¶è¦è¨­ç«‹è² è²¬äºº, ä¾†å°è©²äº‹ä»¶ä½œåˆ†æèˆ‡å›é¥‹. è² è²¬äººä¸æ˜¯å–®å€‹åœ˜éšŠçš„è·è²¬, å¯ä»¥æ˜¯ Devã€Opsã€PMs, etc. ä¾æ“šç›®æ¨™ä¸åŒ, è² è²¬äººä¹Ÿæœƒä¸åŒ. ä¾æ“šç‹€æ³èˆ‡ç›¸é—œåˆ©ç›Šè€…åˆ†äº«å ±å‘Šåˆ†æ.

ä¸Šè¿°å¹¾é»æ˜¯å¾ªç’°ä¸”æŒçºŒåŸ·è¡Œçš„å„ªåŒ–é …ç›®, ä¸æœƒæ˜¯ä¸€æ¬¡æ€§ä»»å‹™.

### Note

åŸæ–‡ :: [Measuring and evaluating Service Level Objectives (SLOs)](https://www.atlassian.com/blog/opsgenie/measuring-and-evaluating-service-level-objectives)

In this context, SLAs (Service Level Agreement) are likely familiar. **<span style="background-color: #ffffcc; color: red">An SLA is a written agreement between the client and the service provider to ensure a healthy level of quality.</span> If specified conditions arenâ€™t met there are consequences, and they are often financial.**

However, the real world isnâ€™t this simple. Service owners are accountable to serve both outside and inside stakeholders. These stakeholders depend on the services to meet their business objectives.Â This is especially common in microservices architectures, where one service is dependent on another.Â **As it doesnâ€™t make sense to have written contracts for everything, <span style="background-color: #ffffcc; color: red">service owners should be held responsible by defining clear objectives.</span>**Â There are no severe penalties if those objectives arenâ€™t met.Â Yet, this doesnâ€™t mean they are there for nothing. There are some consequences, or ratherâ€“ corrective actions, needed to improve those services.

*A simple equation to define SLA and SLO relationship is:*

$$ SLA = SLO + written\ and\ signed\ consequences $$

Letâ€™s focus on 5 key steps while measuring and evaluating SLOs.

##### Set the right objectives

**<span style="background-color: #ffffcc; color: red">Setting the right objectives is the first important step towards building proper SLOs.</span>** There are some important things to consider at this point:

- Identify key metrics (service level indicatorsâ€Šâ€”â€ŠSLIs) from the end-user viewpoint, such as latency
- Make it measurableâ€“ such as 100 ms. latency
- Allow some space (error budget) such as 100 ms. 99.9% of the time
- Be clear on what you promise, for example 99.9% of the time (averaged over 10 minutes), HTTP calls are completed under 100 ms.
- Consider product and business implications because setting the right objectives for SLOs arenâ€™t purely technical as stated theÂ [in SRE Book](https://landing.google.com/sre/book/chapters/service-level-objectives.html).

Although these points are important and seem obvious, it is really hard to identify the right metrics. Talk openly with users and be clear on what is promised.

##### Collect monitoring data

**<span style="background-color: #ffffcc; color: red">Once important metrics have been identified, they need to be collected.</span> This stage depends heavily on SLOs and what the service means to others. Different things may need to be monitored depending on the level of abstraction.** Often what is needed is a monitoring tool like DataDog to collect and visualize the data. These tools allow for aggregation and alerting when the metric reaches the threshold defined.

##### Alert on collected metrics

**Alerting is a critical and a complex job by itself. Filtering out low priority alerts and letting the team know about these are important for the health of on-call.** But these are not the only places where an incident management solution such asÂ *Opsgenie*Â helps. A proper incident management tool does â€œa lotâ€ more than that. It centralizes all alerts from differentÂ monitoring tools in one dashboardÂ and allows users toÂ [categorize important alerts](https://docs.opsgenie.com/docs/filters)Â for later analysis.


##### Create reports from alerts

**<span style="background-color: #ffffcc; color: red">Once all of the alerts are in one place itâ€™s important to setup alert reporting</span>, which makes it easy to see important data points in a structured view.** To report on SLOs, Service and Infrastructure Health Reports are used at Opsgenie which include key indicators that can be used to evaluate metrics and share with customers as a team. Examples of these metrics are mean time to resolve and close incidents per service, Service health percentage (healthy/unhealthy state by outages and disruptions), severity of incidents that arise in a service and the alerts associated with all incidents (so that insight is gained into which monitoring systems reported the incident in which way) and how stakeholders were affected by the service disruptions â€“ whether they were notified in a timely and proper way. The infrastructure health reports provide infrastructure-wide context by allowing stakeholders to see all alerts and incidents across an entire infrastructure in a single view.

##### Evaluate and share the reports

**<span style="background-color: #ffffcc; color: red">Reports mean nothing if left un-evaluated. As they are the written proof of performance on the service level indicators defined internally, and they help to see if SLOs were met or not.</span> Evaluation should include every team member and stakeholder. This means transparency is crucialâ€“ be open about them and share the results with others.** To dig a little bit deeper with analytics tools or create more sophisticated reports for stakeholders, export the reports for easy sharing.


#### SLOs donâ€™t matter if the cycle isnâ€™t repeated

**<span style="background-color: #ffffcc; color: red">Once the cycle is completedâ€“ from creating the objectives and finishing with evaluatingâ€“ the job still isnâ€™t done. It starts all over again. Reevaluate objectives and take corrective actions either by refining the indicators or making services more robust.</span> Clearly examine error budgets to make sure that overachievement is avoided (yes, that is bad too).** It is important to design objectives taking into account that tools and services will fail, because they will.